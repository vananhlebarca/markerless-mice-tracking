{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markerless Mice Tracking for Social Experiments\n",
    "This is an implementation of the pipeline described in the paper. \n",
    "* Input of the pipeline can be a video or a directory containing frames in sequence. \n",
    "* Output of the workflow are two csv files storing coordinates of snout and tailbase of corresponding to two mice: *features_mouse1_ensemble.csv* and *features_mouse2_ensemble.csv*\n",
    "    \n",
    "\n",
    "This pipeline used a pretrained Mask-RCNN model and a pretrained DeepLabCut model provided in subfolders mrcnn_models and dlc_models. However, Mask-RCNN and DeepLabCut models should be retrained on new data if the settings of new videos are different with our setting in the paper. The code to train those models can be found in two Jupyter Notebooks as follows:\n",
    "* *deeplabcut_training.ipynb*\n",
    "* *mrcnn_training.ipynb*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "\n",
    "from mouse.utils import labelmejson_to_png\n",
    "from mouse.utils import video2frames, background_subtraction\n",
    "from mouse.utils import correct_segmentation_errors, mouse_mrcnn_segmentation, check_mrcnn_model_path\n",
    "from mouse.utils import tracking_inference, mask_based_detection, mice_separation\n",
    "from mouse.utils import deeplabcut_detection, ensemble_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path to cage background which is normally imaged before the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_dir='..\\\\videos\\\\BG1.jpg'                                       #----------------update your cage background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path the a new video or frames\n",
    "The input of the pipeline can be a video or a directory containing a sequence of frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to a new video \n",
    "tracking_video_dir = \"..\\\\videos\\\\video1.avi\"                            #----------------update if input is a video\n",
    "\n",
    "# Extracting the video into frames \n",
    "print('The video will be extracted into frames stored in {}'.format(os.path.splitext(tracking_video_dir)[0] + '\\\\images'))\n",
    "video2frames(tracking_video_dir)  \n",
    "\n",
    "frames_dir = os.path.join(os.path.splitext(tracking_video_dir)[0], 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to frames\n",
    "#frames_dir = '..\\\\videos\\\\video1\\images'                                 #-------------------update if input are frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Conventional foreground detection to segment mice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = background_subtraction(frames_dir, background_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Mask-RCNN to segment mice in the failed frames in step 1\n",
    "Update MODEL_PATH for a specific model you want to use. If MODEL_PATH == None, the weights of your last trained model will be loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"mrcnn_models\")\n",
    "\n",
    "# Directory to load weights of a model. If MRCNN_MODEL_PATH=None, the latest weights in MODEL_DIR will be loaded\n",
    "MRCNN_MODEL_PATH = \"..\\\\mrcnn_models\\\\mask_rcnn_mouse_0025.h5\"  # None\n",
    "\n",
    "if MRCNN_MODEL_PATH != None:\n",
    "    if not os.path.exists(MRCNN_MODEL_PATH):\n",
    "        print(\"Please read mrcnn_models/README.md to download our trained model\")\n",
    "    \n",
    "    else:\n",
    "        components = mouse_mrcnn_segmentation(components, frames_dir, background_dir, model_dir=MODEL_DIR, model_path=MRCNN_MODEL_PATH)\n",
    "        \n",
    "elif MRCNN_MODEL_PATH == None:\n",
    "    if check_mrcnn_model_path(MODEL_DIR):\n",
    "        print(\"The latest trained model will be loaded\")\n",
    "        components = mouse_mrcnn_segmentation(components, frames_dir, background_dir, model_dir=MODEL_DIR, model_path=MRCNN_MODEL_PATH)\n",
    "    else:\n",
    "        print(\"Could not find model directory under {}\".format(MODEL_DIR))\n",
    "        print(\"Please follow the pipeline in mrcnn_training.ipynb to train your own model, then run this step again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Suggesting intervention\n",
    "Althought we did not intervene the Mask-RCNN results in the paper, it is recommended to fix very 3 consecutive failed frames. \n",
    "Skip step 3 if you don't want to correct the mistakes.\n",
    "\n",
    "To correct segmentation errors, in Labelme GUI open *fix_dir* and annotate both mice as a label of *mouse*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_dir = os.path.join(os.path.dirname(frames_dir), 'suggested_fix')\n",
    "os.mkdir(fix_dir)\n",
    "\n",
    "suggesting_correction = correct_segmentation_errors(components, fix_dir, frames_dir)\n",
    "\n",
    "if suggesting_correction==0:\n",
    "    print('There are no frames for correcting')\n",
    "else:\n",
    "    print('{} frames in the directory {} are recommended to be corrected using Labelme'.format(suggesting_correction, fix_dir))\n",
    "    ! labelme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert fix_frames back to FG and update components\n",
    "output_dir = os.path.join(os.path.dirname(frames_dir),'FG')        \n",
    "labelmejson_to_png(fix_dir, output_dir)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Tracking inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_dir = os.path.join(os.path.dirname(frames_dir),'FG')\n",
    "tracking_inference(fg_dir, components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5:  Mask-based detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_dir = os.path.join(os.path.dirname(frames_dir),'tracking')\n",
    "features_mouse1_md, features_mouse2_md = mask_based_detection(tracking_dir, components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Deeplabcut detection\n",
    "Update config_dir to load your own trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Separating the video into 2 videos corresponding to two animals.\n",
    "mice_separation(tracking_dir, frames_dir, background_dir)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Detect the key points by Deeplabcut model\n",
    "config_dir = '../dlc_models/dlc_mice_tracking/config.yaml'              #------Update this for your own model\n",
    "\n",
    "if os.path.exists(config_dir):\n",
    "    video_dir = [os.path.join(os.path.dirname(tracking_dir), 'mouse1.avi'),\n",
    "                 os.path.join(os.path.dirname(tracking_dir), 'mouse2.avi')]\n",
    "\n",
    "    features_mouse1_dlc, features_mouse2_dlc = deeplabcut_detection(config_dir, video_dir)\n",
    "else:\n",
    "    print(\"Please read dlc_models/README.md to download our trained model and run this step again\")\n",
    "    print(\"Or follow the pipeline in dlc_training.ipynb to train our own model, then update config_dir and run this step again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7:  Ensemble\n",
    "The output of the pipeline is the coordinates of snout and tailbase of two mice which are saved as features_mouse1_ensemble.csv and features_mouse2_ensemble.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_mouse1_ensemble = ensemble_features(features_mouse1_md, features_mouse1_dlc, tracking_dir, mouse_id=1)    \n",
    "features_mouse2_ensemble = ensemble_features(features_mouse2_md, features_mouse2_dlc, tracking_dir, mouse_id=2) \n",
    "\n",
    "print(\"Pls find the output results in the directory {}\".format(os.path.dirname(frames_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
